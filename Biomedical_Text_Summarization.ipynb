{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Biomedical Text Summarization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM49tnc0w0T3XvDYOCZTb/l",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChandanNaik999/Biomedical-Text-Summarization/blob/main/Biomedical_Text_Summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8f7Nr6OBXY_"
      },
      "source": [
        "<h1><b>Biomedical Text Summarization using K-means and Apriori Method</b></h1>\n",
        "\n",
        "Presentation [here](https://docs.google.com/presentation/d/1YasEWkFZ_rdpzBRvciN-6wF-H_BT392uX7-et5cD1I4/edit?usp=sharing).\n",
        "\n",
        "To run the code: Runtime->Run All\n",
        "(The entire code takes around 30 min to run)\n",
        "\n",
        "Note: Double click the section to open it\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTOfTnd32lFH"
      },
      "source": [
        "<h2><b>Setup</b></h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kF88UqyG_JmY",
        "cellView": "form"
      },
      "source": [
        "#@title Time the code\n",
        "import time\n",
        "last_time = time.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ckpH3hAKU8y"
      },
      "source": [
        "Download the metamap from [here](https://metamap.nlm.nih.gov/MainDownload.shtml). After downloading the zip file upload it to google drive and paste the id in the variable \"file_id\" below "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-va9C2HdpRD7",
        "cellView": "form"
      },
      "source": [
        "#@title Get Metamap binary from Google Drive\n",
        "import requests\n",
        "\n",
        "def download_file_from_google_drive(id, destination):\n",
        "    URL = \"https://drive.google.com/uc?id=\"+id+\"&export=download\"\n",
        "\n",
        "    session = requests.Session()\n",
        "\n",
        "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
        "    token = get_confirm_token(response)\n",
        "\n",
        "    if token:\n",
        "        params = { 'id' : id, 'confirm' : token }\n",
        "        response = session.get(URL, params = params, stream = True)\n",
        "\n",
        "    save_response_content(response, destination)    \n",
        "\n",
        "def get_confirm_token(response):\n",
        "    for key, value in response.cookies.items():\n",
        "        if key.startswith('download_warning'):\n",
        "            return value\n",
        "\n",
        "    return None\n",
        "\n",
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk: # filter out keep-alive new chunks\n",
        "                f.write(chunk)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    file_id = '1DadjKQVk8v9ukKnund3O1ThMlEASc2HE'\n",
        "    destination = '/content/public_mm_linux_main_2016.tar.bz2'\n",
        "    download_file_from_google_drive(file_id, destination)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCIKwgS-__9t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "922e99cf-d9e4-451c-9155-305c110ce1d8"
      },
      "source": [
        "#@title Metamap Unzip and Setup\n",
        "%%shell\n",
        "\n",
        "#cp /content/gdrive/My\\ Drive/public_mm_linux_main_2016.tar.bz2 /content/\n",
        "tar -xf public_mm_linux_main_2016.tar.bz2\n",
        "export JAVA_HOME=/usr/\n",
        "export PATH=$PATH:/content/public_mm/bin"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7xNVo8qIS2g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c15df512-1652-41dc-8d7d-fbbf652886b3"
      },
      "source": [
        "cd public_mm/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/public_mm\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "d3dIu5PMBl96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d22b08a-b17e-4758-9356-1095e102742f"
      },
      "source": [
        "#@title Metamap Installation\n",
        "%%shell\n",
        "\n",
        "#! /bin/bash\n",
        "# install.sh - Public MetaMap Install Program\n",
        "#\n",
        "# public_mm/bin/install.sh, Wed Jul 16 10:21:21 2008, edit by Will Rogers\n",
        "#\n",
        "# This script reads environment variables BASEDIR and JAVA_HOME from\n",
        "# parent process.\n",
        "# \n",
        "if [ -x ./bin/date ]; then\n",
        "  echo \"$0 started `./bin/date`\"  > ./install.log\n",
        "else\n",
        "  echo \"$0 started `date`\"  > ./install.log\n",
        "fi\n",
        "if [ -z $OS ]; then\n",
        "    if [ -x ./bin/uname ]; then\n",
        "\tOS=`./bin/uname`\n",
        "    else\n",
        "\tOS=`uname`\n",
        "    fi\n",
        "fi\n",
        "case $OS in\n",
        "    \"Linux\")\n",
        "        # GNU libc version test\n",
        "\tREQ_MAJOR_VERSION=2\n",
        "\tREQ_MINOR_VERSION=12\n",
        "\techo \"glib minimun required version: $REQ_MAJOR_VERSION.$REQ_MINOR_VERSION\"   >> ./install.log\n",
        "\techo \"glib minimun required version: $REQ_MAJOR_VERSION.$REQ_MINOR_VERSION\" \n",
        "\t;;\n",
        "    \"Windows_NT\" | \"MINGW32_NT-5.1\" | \"CYGWIN_NT-6.1\")\n",
        "\tPATH=$PATH:./bin\n",
        "\t;;\n",
        "esac\n",
        "\n",
        "# load installrc if present.\n",
        "if [ -r .installrc ]; then\n",
        "    source .installrc\n",
        "fi\n",
        "BASEDIR=$PWD\n",
        "BINDIR=$BASEDIR/bin\n",
        "\n",
        "if [ \"$JAVA_HOME\" = \"\" ]; then\n",
        "  javaprog=`which java`\n",
        "  if [ $? -eq 0 ]; then\n",
        "    javabindir=`dirname $javaprog`\n",
        "    RC_JAVA_HOME=`dirname $javabindir`\n",
        "  else\n",
        "    RC_JAVA_HOME=\"\"\n",
        "  fi\n",
        "else\n",
        "  RC_JAVA_HOME=$JAVA_HOME\n",
        "fi\n",
        "JAVA_HOME=$RC_JAVA_HOME #added line\n",
        "# echo  \"Enter home path of JRE (JDK) [$RC_JAVA_HOME]: \" \n",
        "# read JAVA_HOME\n",
        "# if [ \"$JAVA_HOME\" = \"\" ]; then\n",
        "#    JAVA_HOME=$RC_JAVA_HOME\n",
        "# fi     \n",
        "\n",
        "echo Using $JAVA_HOME for JAVA_HOME.\n",
        "echo Using $JAVA_HOME for JAVA_HOME. >> ./install.log\n",
        "echo \"\"\n",
        "\n",
        "# setup WSD main and logging configuration files\n",
        "if [ $OS = \"MINGW32_NT-5.1\" ]; then\n",
        "    sed -e \"s:@@basedir@@:\\.\\.:g\" $BASEDIR/WSD_Server/config/disambServer.cfg.in > $BASEDIR/WSD_Server/config/disambServer.cfg\n",
        "else \n",
        "    sed -e \"s:@@basedir@@:$BASEDIR:g\" $BASEDIR/WSD_Server/config/disambServer.cfg.in > $BASEDIR/WSD_Server/config/disambServer.cfg\n",
        "fi\n",
        "if [ -r $BASEDIR/WSD_Server/config/disambServer.cfg ]; then\n",
        "  echo $BASEDIR/WSD_Server/config/disambServer.cfg generated\n",
        "fi\n",
        "if [ $OS = \"MINGW32_NT-5.1\" ]; then\n",
        "sed -e \"s:@@basedir@@:\\.\\.:g\" $BASEDIR/WSD_Server/config/log4j.properties.in > $BASEDIR/WSD_Server/config/log4j.properties\n",
        "else \n",
        "sed -e \"s:@@basedir@@:$BASEDIR:g\" $BASEDIR/WSD_Server/config/log4j.properties.in > $BASEDIR/WSD_Server/config/log4j.properties\n",
        "fi\n",
        "if [ -r $BASEDIR/WSD_Server/config/log4j.properties ]; then\n",
        "  echo $BASEDIR/WSD_Server/config/log4j.properties generated\n",
        "fi\n",
        "\n",
        "if [ ! -d $BASEDIR/WSD_Server/log ]; then\n",
        "  mkdir $BASEDIR/WSD_Server/log\n",
        "  if [ -d $BASEDIR/WSD_Server/log ]; then\n",
        "    echo Created directory $BASEDIR/WSD_Server/log\n",
        "    echo Created directory $BASEDIR/WSD_Server/log >> ./install.log\n",
        "  fi\n",
        "fi\n",
        "\n",
        "if [ ! -d $BASEDIR/MedPost-SKR/Tagger_server/log ]; then\n",
        "  mkdir $BASEDIR/MedPost-SKR/Tagger_server/log\n",
        "  if [ -d $BASEDIR/MedPost-SKR/Tagger_server/log ]; then\n",
        "    echo Created directory $BASEDIR/Tagger_server/log\n",
        "    echo Created directory $BASEDIR/Tagger_server/log >> ./install.log\n",
        "  fi\n",
        "fi\n",
        "\n",
        "wsd_method_value=\"['AEC_METHOD']\"\n",
        "\n",
        "echo \"Setting up bin directory scripts:\"\n",
        "echo \"Setting up bin directory scripts:\" >> ./install.log\n",
        "# Setup all scripts generated from templates\n",
        "for binscriptbase in $BASEDIR/bin/*.in\n",
        "do\n",
        "  binscript=`basename $binscriptbase .in`\n",
        "  binscripttmp1=${binscript}.tmp1\n",
        "  binscripttmp0=${binscript}.tmp0\n",
        "  sed -e \"s:@@basedir@@:$BASEDIR:g\" $binscriptbase > $BINDIR/$binscripttmp1\n",
        "  sed -e \"s:@@java_home@@:$JAVA_HOME:g\" $BINDIR/$binscripttmp1 > $BINDIR/$binscripttmp0\n",
        "  sed -e \"s:@@wsd_methods@@:$wsd_method_value:g\" $BINDIR/$binscripttmp0 > $BINDIR/$binscript\n",
        "  chmod +x $BINDIR/$binscript\n",
        "  if [ -x $BINDIR/$binscript ]; then\n",
        "    rm $BINDIR/$binscripttmp0\n",
        "    rm $BINDIR/$binscripttmp1\n",
        "    echo $BINDIR/$binscript generated.\n",
        "    echo $BINDIR/$binscript generated. >> ./install.log\n",
        "  fi\n",
        "done\n",
        "\n",
        "echo \"Setting up test suite:\"\n",
        "echo \"Setting up test suite:\" >> ./install.log 2>&1\n",
        "\n",
        "# Setup test script (runTest_<year>.sh runTest_<year>.sh)\n",
        "for rtscripttmp in $BASEDIR/TestSuite/runTest_*.in\n",
        "do\n",
        "  rtscript=`basename $rtscripttmp .in`\n",
        "\n",
        "  sed -e \"s:@@basedir@@:$BASEDIR:g\" $rtscripttmp > $BASEDIR/TestSuite/$rtscript\n",
        "  chmod +x $BASEDIR/TestSuite/$rtscript\n",
        "  if [ -x $BASEDIR/TestSuite/$rtscript ]; then\n",
        "    echo $BASEDIR/TestSuite/$rtscript generated.\n",
        "    echo $BASEDIR/TestSuite/$rtscript generated. >> ./install.log\n",
        "  fi\n",
        "done\n",
        "\n",
        "# check for presence of lexicon, databases, tagger, and optional WSD.\n",
        "echo \"Checking for required datafiles\"\n",
        "echo \"Checking for required datafiles\" >> ./install.log\n",
        "MISSINGFILES=0\n",
        "\n",
        "\n",
        "checkforfile ()\n",
        "{\n",
        "    FLIST=`find $CHKDIR -name $CHKFILE -print | wc -l`\n",
        "    if [ $FLIST == 0 ]; then\n",
        "\techo \"Warning: the $CHKFILE file is missing in $CHKDIR, cannot ensure correct operation of MetaMap without file!\"\n",
        "\techo \"Warning: the $CHKFILE file is missing in $CHKDIR, cannot ensure correct operation of MetaMap without file!\" >> ./install.log\n",
        "\techo \"\"\n",
        "\tMISSINGFILES=`expr $MISSINGFILES + 1`\n",
        "    fi\n",
        "}\n",
        "\n",
        "cd $BASEDIR\n",
        "\n",
        "# DB Lexicon files\n",
        "case $OS in\n",
        "    \"Windows_NT\" | \"MINGW32_NT-5.1\" | \"CYGWIN_NT-6.1\")\n",
        "\tCHKDIR=./DB/\\*.strict\n",
        "\t;;\n",
        "    *)\n",
        "\tCHKDIR=./lexicon/data\n",
        "\t;;\n",
        "esac\n",
        "CHKFILE=dm_vars\n",
        "checkforfile\n",
        "CHKFILE=im_vars\n",
        "checkforfile\n",
        "CHKFILE=lex_form\n",
        "checkforfile\n",
        "CHKFILE=lex_rec\n",
        "checkforfile\n",
        "CHKFILE=norm_prefix\n",
        "checkforfile\n",
        "\n",
        "# MetaMap DB files\n",
        "CHKDIR=./DB\n",
        "CHKFILE=DB.\\*.base\n",
        "checkforfile\n",
        "CHKFILE=DB.\\*.[smr]\\* \n",
        "checkforfile\n",
        "\n",
        "\n",
        "# MedPost SKR tagger directory\n",
        "TAGDIR=`find . -name MedPost-SKR -print | wc -l`\n",
        "if [ $TAGDIR -eq 0 ]; then\n",
        "  echo \"Warning: Tagger directory is missing, cannot ensure correct operation of MetaMap without it!\"\n",
        "  echo \"Warning: Tagger directory is missing, cannot ensure correct operation of MetaMap without it!\" >> ./install.log\n",
        "  echo \"\"\n",
        "  MISSINGFILES=`expr $MISSINGFILES + 1`\n",
        "else\n",
        "  # MedPost SKR tagger files\n",
        "  TAGFILES=`find ./MedPost-SKR/data -name \\*.cur -print | wc -l`\n",
        "  if [ $TAGFILES -eq 0 ]; then\n",
        "    echo \"Warning: Tagger index files are missing, cannot ensure correct operation of MetaMap without them!\"\n",
        "    echo \"Warning: Tagger index files are missing, cannot ensure correct operation of MetaMap without them!\" >> ./install.log\n",
        "    echo \"\"\n",
        "    MISSINGFILES=`expr $MISSINGFILES + 1`\n",
        "  fi\n",
        "fi\n",
        "\n",
        "echo \"Checking for optional datafiles (WSD)\"\n",
        "echo \"Checking for optional datafiles (WSD)\" >> ./install.log 2>&1 2>&1\n",
        "MISSINGOPTIONS=0\n",
        "\n",
        "# WSD Server directory\n",
        "WSDDIR=`find . -name WSD_Server -print | wc -l`\n",
        "if [ $WSDDIR -eq 0 ]; then\n",
        "  echo \"Warning: WSD Server directory is missing, MetaMap will not have WSD support without it!\"\n",
        "  echo \"Warning: WSD Server directory is missing, MetaMap will not have WSD support without it!\" >> ./install.log\n",
        "  echo \"\"\n",
        "  MISSINGOPTIONS=`expr $MISSINGOPTIONS + 1`\n",
        "else \n",
        "  # WSD Server files\n",
        "  # WSDFILES=`find ./WSD_Server -name wstv\\* -print | wc -l`\n",
        "  # if [ $WSDFILES == 0 ]; then\n",
        "  #   echo \"Warning: WSD Server index files are missing, MetaMap will not have WSD support without it!\"\n",
        "  #   echo \"Warning: WSD Server index files are missing, MetaMap will not have WSD support without it!\" >> ./install.log\n",
        "  #   echo \"\"\n",
        "  #   MISSINGOPTIONS=`expr $MISSINGOPTIONS + 1`\n",
        "  # fi\n",
        "  echo \"\"\n",
        "fi\n",
        "\n",
        "# echo BASEDIR=$BASEDIR > .installrc\n",
        "# echo JAVA_HOME=$JAVA_HOME >> .installrc\n",
        "echo \"MISSINGOPTIONS=$MISSINGOPTIONS\" >> ./install.log\n",
        "if [ $MISSINGOPTIONS -ge 1 ]; then\n",
        "  echo \"!! WARNING: Some optional datafiles are missing, see install.log for more information. !!\"\n",
        "  echo \"!! WARNING: Some optional datafiles are missing, see install.log for more information. !!\" >> ./install.log\n",
        "fi\n",
        "\n",
        "echo \"MISSINGFILES=$MISSINGFILES\" >> ./install.log\n",
        "if [ $MISSINGFILES -ge 1 ]; then\n",
        "  echo \"!! WARNING: Some necessary datafiles are missing, see install.log for more information. !!\"\n",
        "  echo \"!! WARNING: Some necessary datafiles are missing, see install.log for more information. !!\" >> ./install.log\n",
        "else\n",
        "  echo Public MetaMap Install complete.\n",
        "  echo Public MetaMap Install complete. >> ./install.log\n",
        "fi\n",
        "echo \"$0 ended `date`\"  >> ./install.log\n",
        "\n",
        "export BASEDIR\n",
        "\n",
        "if [ -f $BASEDIR/bin/install_dfb.sh ]; then \n",
        "  echo  \"Would like to use a custom data set with MetaMap (use data file builder)? [yN]:\"\n",
        "  read RESPONSE\n",
        "  if [ \"$RESPONSE\" = \"y\" ]; then\n",
        "     echo \"running Data File Builder Install...\"\n",
        "     . $BASEDIR/bin/install_dfb.sh\n",
        "  fi\n",
        "fi\n",
        "\n",
        "if [ -f $BASEDIR/bin/install_src.sh ]; then \n",
        "    echo \"Running MetaMap source development environment setup...\"\n",
        "    . $BASEDIR/bin/install_src.sh\n",
        "fi\n",
        "\n",
        "if [ -f $BASEDIR/bin/install_javaapi.sh ]; then \n",
        "    echo \"Running MetaMap Java API development environment setup...\"\n",
        "    . $BASEDIR/bin/install_javaapi.sh\n",
        "fi\n",
        "\n",
        "if [ -f $BASEDIR/bin/install_uima.sh ]; then \n",
        "    echo \"Running MetaMap UIMA API development environment setup...\"\n",
        "    . $BASEDIR/bin/install_uima.sh\n",
        "fi\n",
        "\n",
        "echo \"\"\n",
        "echo \"Public MetaMap Install Settings:\"\n",
        "echo \"\"\n",
        "echo \"Public MetaMap basedir: $BASEDIR\"\n",
        "echo \"Public MetaMap Program Dir: $BINDIR\"\n",
        "echo \"Java Home dir: $JAVA_HOME\"\n",
        "\n",
        "echo \"Public MetaMap basedir: $BASEDIR\" >> ./install.log\n",
        "echo \"Public MetaMap Program Dir: $BINDIR\" >> ./install.log\n",
        "echo \"Java Home dir: $JAVA_HOME\" >> ./install.log\n",
        "echo \"\"\n",
        "\n",
        "exit 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "glib minimun required version: 2.12\n",
            "Using /usr for JAVA_HOME.\n",
            "\n",
            "/content/public_mm/WSD_Server/config/disambServer.cfg generated\n",
            "/content/public_mm/WSD_Server/config/log4j.properties generated\n",
            "Created directory /content/public_mm/WSD_Server/log\n",
            "Created directory /content/public_mm/Tagger_server/log\n",
            "Setting up bin directory scripts:\n",
            "/content/public_mm/bin/metamap16 generated.\n",
            "/content/public_mm/bin/metamap2016.TEMPLATE generated.\n",
            "/content/public_mm/bin/metamap generated.\n",
            "/content/public_mm/bin/skrmedpostctl generated.\n",
            "/content/public_mm/bin/SKRrun.16 generated.\n",
            "/content/public_mm/bin/uninstall.sh generated.\n",
            "/content/public_mm/bin/wsdserverctl generated.\n",
            "Setting up test suite:\n",
            "/content/public_mm/TestSuite/runTest_2009.sh generated.\n",
            "/content/public_mm/TestSuite/runTest_2010.sh generated.\n",
            "/content/public_mm/TestSuite/runTest_2011.bat generated.\n",
            "/content/public_mm/TestSuite/runTest_2011.sh generated.\n",
            "/content/public_mm/TestSuite/runTest_2012.sh generated.\n",
            "Checking for required datafiles\n",
            "Checking for optional datafiles (WSD)\n",
            "\n",
            "Public MetaMap Install complete.\n",
            "\n",
            "Public MetaMap Install Settings:\n",
            "\n",
            "Public MetaMap basedir: /content/public_mm\n",
            "Public MetaMap Program Dir: /content/public_mm/bin\n",
            "Java Home dir: /usr\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zriPW7zsDjM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b88552ff-1ee7-46f0-eea4-511b570f9972"
      },
      "source": [
        "!git clone https://github.com/ChandanNaik999/pymetamap.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'pymetamap'...\n",
            "remote: Enumerating objects: 25, done.\u001b[K\n",
            "remote: Counting objects: 100% (25/25), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 204 (delta 13), reused 13 (delta 7), pack-reused 179\u001b[K\n",
            "Receiving objects: 100% (204/204), 45.38 KiB | 2.06 MiB/s, done.\n",
            "Resolving deltas: 100% (120/120), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIz9mF75jT32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63c4ff46-ae2e-4c37-9f9f-f285d7953855"
      },
      "source": [
        "cd pymetamap"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/public_mm/pymetamap\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFGQiPcbjXnY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83b7f2e5-a602-49f9-a669-43fceab40d43"
      },
      "source": [
        "!python setup.py install --user"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating pymetamap.egg-info\n",
            "writing pymetamap.egg-info/PKG-INFO\n",
            "writing dependency_links to pymetamap.egg-info/dependency_links.txt\n",
            "writing top-level names to pymetamap.egg-info/top_level.txt\n",
            "writing manifest file 'pymetamap.egg-info/SOURCES.txt'\n",
            "writing manifest file 'pymetamap.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib\n",
            "creating build/lib/pymetamap\n",
            "copying pymetamap/__init__.py -> build/lib/pymetamap\n",
            "copying pymetamap/ConceptLite.py -> build/lib/pymetamap\n",
            "copying pymetamap/SubprocessBackend.py -> build/lib/pymetamap\n",
            "copying pymetamap/SubprocessBackendLite.py -> build/lib/pymetamap\n",
            "copying pymetamap/Concept.py -> build/lib/pymetamap\n",
            "copying pymetamap/MetaMapLite.py -> build/lib/pymetamap\n",
            "copying pymetamap/MetaMap.py -> build/lib/pymetamap\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/pymetamap\n",
            "copying build/lib/pymetamap/__init__.py -> build/bdist.linux-x86_64/egg/pymetamap\n",
            "copying build/lib/pymetamap/ConceptLite.py -> build/bdist.linux-x86_64/egg/pymetamap\n",
            "copying build/lib/pymetamap/SubprocessBackend.py -> build/bdist.linux-x86_64/egg/pymetamap\n",
            "copying build/lib/pymetamap/SubprocessBackendLite.py -> build/bdist.linux-x86_64/egg/pymetamap\n",
            "copying build/lib/pymetamap/Concept.py -> build/bdist.linux-x86_64/egg/pymetamap\n",
            "copying build/lib/pymetamap/MetaMapLite.py -> build/bdist.linux-x86_64/egg/pymetamap\n",
            "copying build/lib/pymetamap/MetaMap.py -> build/bdist.linux-x86_64/egg/pymetamap\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pymetamap/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pymetamap/ConceptLite.py to ConceptLite.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pymetamap/SubprocessBackend.py to SubprocessBackend.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pymetamap/SubprocessBackendLite.py to SubprocessBackendLite.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pymetamap/Concept.py to Concept.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pymetamap/MetaMapLite.py to MetaMapLite.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pymetamap/MetaMap.py to MetaMap.cpython-36.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying pymetamap.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying pymetamap.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying pymetamap.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying pymetamap.egg-info/not-zip-safe -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying pymetamap.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "creating dist\n",
            "creating 'dist/pymetamap-0.2-py3.6.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing pymetamap-0.2-py3.6.egg\n",
            "creating /root/.local/lib/python3.6/site-packages/pymetamap-0.2-py3.6.egg\n",
            "Extracting pymetamap-0.2-py3.6.egg to /root/.local/lib/python3.6/site-packages\n",
            "Adding pymetamap 0.2 to easy-install.pth file\n",
            "\n",
            "Installed /root/.local/lib/python3.6/site-packages/pymetamap-0.2-py3.6.egg\n",
            "Processing dependencies for pymetamap==0.2\n",
            "Finished processing dependencies for pymetamap==0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kz02ff0Jjb8D"
      },
      "source": [
        "from pymetamap import MetaMap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bzwvX7Q7URT"
      },
      "source": [
        "**Starting the SKR/Medpost Part-of-Speech Tagger Server:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyXq1phA7Ag4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4331b56-67b2-4434-8feb-210c702f2abe"
      },
      "source": [
        "!/content/public_mm/bin/skrmedpostctl start"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "$Starting skrmedpostctl: \n",
            "started.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1qc2kC_7f2T"
      },
      "source": [
        "**You can determine if the server are running by the command:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLqBghiR7Frd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78f23d58-13ee-499d-a3c2-64a3cb956118"
      },
      "source": [
        "!ps -ef | grep java"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root         234       1  0 12:23 ?        00:00:00 java -Dtaggerserver.port=1795 -DlexFile=/content/public_mm/MedPost-SKR/data/lexDB.serial -DngramOne=/content/public_mm/MedPost-SKR/data/ngramOne.serial -cp /content/public_mm/MedPost-SKR/Tagger_server/lib/taggerServer.jar:/content/public_mm/MedPost-SKR/Tagger_server/lib/mps.jar taggerServer\n",
            "root         235      56  0 12:23 ?        00:00:00 /bin/bash -c ps -ef | grep java\n",
            "root         237     235  0 12:23 ?        00:00:00 grep java\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "98OAao8L9YBJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75134ab8-2c25-4d2b-9a0b-c0f69cc8015b"
      },
      "source": [
        "#@title UMLS Concept mapping using Pymetamap\n",
        "# metamap test script\n",
        "# to run this script ensure that the \"SKR/Medpost Part-of-Speech Tagger Server\" has started\n",
        "from pymetamap import MetaMap\n",
        "mm = MetaMap.get_instance('/content/public_mm/bin/metamap16')\n",
        "\n",
        "# sents = ['Heart Attack', 'John had a huge heart attack']\n",
        "sents = ['developing world 15 cancers due infections helicobacter pylori hepatitis b ','hepatitis c human papillomavirus ','infection epsteinâ€“barr virus human immunodeficiency virus hiv']\n",
        "concepts,error = mm.extract_concepts(sents,[1,2,3])\n",
        "for concept in concepts:\n",
        "  print(concept)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ConceptMMI(index='1', mm='MMI', score='32.62', preferred_name='Hepatitis B', cui='C0019163', semtypes='[dsyn]', trigger='[\"HEPATITIS B\"-tx-1-\"hepatitis b\"-noun-0,\"Infectious Hepatitis b\"-tx-1-\"infections hepatitis b\"-noun-0]', location='TX', pos_info='64/11;33/10,64/11', tree_codes='C02.256.430.400;C02.440.435;C06.552.380.705.437')\n",
            "ConceptMMI(index='1', mm='MMI', score='16.08', preferred_name='Helicobacter pylori', cui='C0079488', semtypes='[bact]', trigger='[\"HELICOBACTER PYLORI\"-tx-1-\"helicobacter pylori\"-noun-0]', location='TX', pos_info='44/19', tree_codes='B03.440.500.550;B03.660.150.280.550')\n",
            "ConceptMMI(index='1', mm='MMI', score='6.60', preferred_name='Infection', cui='C3714514', semtypes='[patf]', trigger='[\"Infections\"-tx-1-\"infections\"-noun-0]', location='TX', pos_info='33/10', tree_codes='C01.539')\n",
            "ConceptMMI(index='1', mm='MMI', score='3.45', preferred_name='Cancer Genus', cui='C0998265', semtypes='[euka]', trigger='[\"Cancer\"-tx-1-\"cancers\"-noun-0]', location='TX', pos_info='21/7', tree_codes='')\n",
            "ConceptMMI(index='1', mm='MMI', score='3.45', preferred_name='Due', cui='C3146286', semtypes='[idcn]', trigger='[\"Due\"-tx-1-\"due\"-adj-0]', location='TX', pos_info='29/3', tree_codes='')\n",
            "ConceptMMI(index='1', mm='MMI', score='3.45', preferred_name='Due to', cui='C0678226', semtypes='[ftcn]', trigger='[\"due\"-tx-1-\"due\"-adj-0]', location='TX', pos_info='29/3', tree_codes='')\n",
            "ConceptMMI(index='1', mm='MMI', score='3.45', preferred_name='Malignant Neoplasms', cui='C0006826', semtypes='[neop]', trigger='[\"CANCER\"-tx-1-\"cancers\"-noun-0]', location='TX', pos_info='21/7', tree_codes='')\n",
            "ConceptMMI(index='1', mm='MMI', score='3.45', preferred_name='Primary malignant neoplasm', cui='C1306459', semtypes='[neop]', trigger='[\"Cancer\"-tx-1-\"cancers\"-noun-0]', location='TX', pos_info='21/7', tree_codes='')\n",
            "ConceptMMI(index='1', mm='MMI', score='3.45', preferred_name='World', cui='C2700280', semtypes='[popg]', trigger='[\"World\"-tx-1-\"world\"-noun-0]', location='TX', pos_info='12/5', tree_codes='')\n",
            "ConceptMMI(index='2', mm='MMI', score='16.12', preferred_name='Hepatitis C', cui='C0019196', semtypes='[dsyn]', trigger='[\"Hepatitis C\"-tx-1-\"hepatitis c\"-noun-0]', location='TX', pos_info='1/11', tree_codes='C02.440.440;C02.782.350.350;C06.552.380.705.440')\n",
            "ConceptMMI(index='2', mm='MMI', score='12.96', preferred_name='Hepatitis C virus', cui='C0220847', semtypes='[virs]', trigger='[\"Hepatitis C\"-tx-1-\"hepatitis c\"-noun-0]', location='TX', pos_info='1/11', tree_codes='B04.450.410;B04.820.250.475')\n",
            "ConceptMMI(index='2', mm='MMI', score='3.68', preferred_name='Human Papillomavirus', cui='C0021344', semtypes='[virs]', trigger='[\"HUMAN PAPILLOMAVIRUS\"-tx-1-\"human papillomavirus\"-noun-0]', location='TX', pos_info='13/20', tree_codes='')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPL0S_lS3s5I"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnHjRNttCgCY"
      },
      "source": [
        "<h2><b>Actual Code Starts Here</b></h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZAH03FH99Al",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "bc7b4f84-6e17-4fd7-98ca-39d27af66ddc"
      },
      "source": [
        "#@title  Install Pip packages\n",
        "!pip install wikipedia-api\n",
        "!pip install rouge\n",
        "\n",
        "# time taken\n",
        "print(time.perf_counter() - last_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wikipedia-api\n",
            "  Downloading https://files.pythonhosted.org/packages/ef/3d/289963bbf51f8d00cdf7483cdc2baee25ba877e8b4eb72157c47211e3b57/Wikipedia-API-0.5.4.tar.gz\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from wikipedia-api) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->wikipedia-api) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->wikipedia-api) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->wikipedia-api) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->wikipedia-api) (2.10)\n",
            "Building wheels for collected packages: wikipedia-api\n",
            "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia-api: filename=Wikipedia_API-0.5.4-cp36-none-any.whl size=13462 sha256=b7bf2dc52321c3150596df4c5aea6fad58b74a5c070962c2a07aeb01e57db298\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/40/42/ba1d497f3712281b659dd65b566fc868035c859239571a725a\n",
            "Successfully built wikipedia-api\n",
            "Installing collected packages: wikipedia-api\n",
            "Successfully installed wikipedia-api-0.5.4\n",
            "Collecting rouge\n",
            "  Downloading https://files.pythonhosted.org/packages/43/cc/e18e33be20971ff73a056ebdb023476b5a545e744e3fc22acd8c758f1e0d/rouge-1.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from rouge) (1.15.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.0\n",
            "-1606478298.278973\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFASSTYtCqZn",
        "cellView": "form"
      },
      "source": [
        "#@title Get Data\n",
        "import wikipediaapi\n",
        "\n",
        "\n",
        "def get_article(topic=\"apple\"):\n",
        "    # f = open('/content/content.txt', 'r')\n",
        "    # content = f.read()\n",
        "    # f.close()\n",
        "\n",
        "    # f = open('/content/summary.txt', 'r')\n",
        "    # summary = f.read()\n",
        "    # f.close()\n",
        "\n",
        "    # return content, summary\n",
        "    \n",
        "\n",
        "    # get data from wiki\n",
        "    wiki = wikipediaapi.Wikipedia(language='en', extract_format=wikipediaapi.ExtractFormat.WIKI)\n",
        "    try:\n",
        "        p_wiki = wiki.page(topic)\n",
        "    except:\n",
        "        print(\"Page \" + topic + \" Exists: False\")\n",
        "        quit()\n",
        "\n",
        "    return p_wiki.text, p_wiki.summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "sZ21Cpg2DA2W"
      },
      "source": [
        "#@title Cosine\n",
        "from rouge import Rouge\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# from formatting import gen_serie\n",
        "# from tokenizer import textblob_tokenizer\n",
        "\n",
        "\n",
        "def cosine(texts, ref):\n",
        "    vec = TfidfVectorizer(tokenizer=textblob_tokenizer,\n",
        "                          stop_words='english',\n",
        "                          use_idf=True)\n",
        "    matrix = vec.fit_transform(texts)\n",
        "\n",
        "    cosine_similarities = cosine_similarity(matrix[0:1], matrix).flatten()\n",
        "\n",
        "    nb_sentences_in_base_summary = len(ref.split('.'))\n",
        "\n",
        "    cosine_similarities = list(cosine_similarities)\n",
        "    cos_results = []\n",
        "    for i in range(0, nb_sentences_in_base_summary):\n",
        "        n = cosine_similarities.index(max(cosine_similarities))\n",
        "        cos_results.append(texts[n])\n",
        "        del cosine_similarities[n]\n",
        "\n",
        "    res = ' '.join(cos_results)\n",
        "\n",
        "    r = Rouge()\n",
        "    rouge = r.get_scores(res, ref)\n",
        "\n",
        "    return gen_serie('Cosine Similarity', rouge, res, version = 'rouge-l'), gen_serie('Cosine Similarity', rouge, res, version = 'rouge-1'), gen_serie('Cosine Similarity', rouge, res, version = 'rouge-2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "kCjCiT33xdaE"
      },
      "source": [
        "#@title TextRank Algorithm\n",
        "import heapq\n",
        "import re\n",
        "\n",
        "# import nltk\n",
        "\n",
        "\n",
        "def compute(sentences, stopwords, formatted):\n",
        "    word_frequencies = {}\n",
        "    sentence_scores = {}\n",
        "\n",
        "    for word in nltk.word_tokenize(formatted):\n",
        "        if word not in stopwords:\n",
        "            if word not in word_frequencies.keys():\n",
        "                word_frequencies[word] = 1\n",
        "            else:\n",
        "                word_frequencies[word] += 1\n",
        "\n",
        "    maximum_frequency = max(word_frequencies.values())\n",
        "\n",
        "    for word in word_frequencies.keys():\n",
        "        word_frequencies[word] = (word_frequencies[word] / maximum_frequency)\n",
        "\n",
        "    for sent in sentences:\n",
        "        for word in nltk.word_tokenize(sent.lower()):\n",
        "            if word in word_frequencies.keys():\n",
        "                if len(sent.split(' ')) < 30:\n",
        "                    if sent not in sentence_scores.keys():\n",
        "                        sentence_scores[sent] = word_frequencies[word]\n",
        "                    else:\n",
        "                        sentence_scores[sent] += word_frequencies[word]\n",
        "    return sentence_scores\n",
        "\n",
        "\n",
        "def text_rank(raw, text, ref):\n",
        "    nb = len(ref.split('.'))\n",
        "\n",
        "    formatted = re.sub('[^a-zA-Z]', ' ', raw)\n",
        "    formatted = re.sub(r'\\s+', ' ', formatted)\n",
        "\n",
        "    scores = compute(text, nltk.corpus.stopwords.words('english'), formatted)\n",
        "    sentences = heapq.nlargest(nb, scores, key=scores.get)\n",
        "\n",
        "    return ' '.join(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujDr2LL1C3ND",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "eef8dd84-5406-4a3e-c044-9f683cad3f46"
      },
      "source": [
        "#@title Split and Tokenize sentences\n",
        "import string\n",
        "\n",
        "# import nltk\n",
        "from textblob import TextBlob\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def tokenize(raw):\n",
        "    #3.1.2-3.1.3\n",
        "    # tokenize the para to sentences\n",
        "    sent = nltk.sent_tokenize(raw)\n",
        "    return sent\n",
        "\n",
        "def remove_punctuation_to_lower_case(sent):\n",
        "    # remove punctuation marks and convert to lower case\n",
        "    return list(map(lambda x: x.translate(dict.fromkeys(list(map(lambda x:ord(x),list(string.punctuation))), \"\")).lower(), sent))\n",
        "\n",
        "\n",
        "def textblob_tokenizer(str_input):\n",
        "    blob = TextBlob(str_input.lower())\n",
        "    tokens = blob.words\n",
        "    words = [token.stem() for token in tokens]\n",
        "    return words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMC3WN9bw3ko",
        "cellView": "form"
      },
      "source": [
        "#@title Cosine Similarity { form-width: \"200px\" }\n",
        "from math import sqrt\n",
        "\n",
        "\n",
        "def dot_product(v1, v2):\n",
        "    \"\"\"Get the dot product of the two vectors.\n",
        "\n",
        "    if A = [a1, a2, a3] && B = [b1, b2, b3]; then\n",
        "    dot_product(A, B) == (a1 * b1) + (a2 * b2) + (a3 * b3)\n",
        "    true\n",
        "\n",
        "    Input vectors must be the same length.\n",
        "\n",
        "    \"\"\"\n",
        "    return sum(a * b for a, b in zip(v1, v2))\n",
        "\n",
        "\n",
        "def magnitude(vector):\n",
        "    \"\"\"Returns the numerical length / magnitude of the vector.\"\"\"\n",
        "    return sqrt(dot_product(vector, vector))\n",
        "\n",
        "\n",
        "def similarity(v1, v2):\n",
        "    \"\"\"Ratio of the dot product & the product of the magnitudes of vectors.\"\"\"\n",
        "    return dot_product(v1, v2) / (magnitude(v1) * magnitude(v2) + .00000000001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqchISL72ikW",
        "cellView": "form"
      },
      "source": [
        "#@title Kmeans with Cosine Similarity { form-width: \"200px\" }\n",
        "import random\n",
        "class CosineKMeans(object):\n",
        "    \"\"\"K-Means clustering. Uses cosine similarity as the distance function.\"\"\"\n",
        "\n",
        "    def __init__(self, k, vectors,iter):\n",
        "        assert len(vectors) >= k\n",
        "        random.seed(20)\n",
        "        self.centers = random.sample(vectors, k)\n",
        "        self.clusters = [[] for c in self.centers]\n",
        "        self.vectors = vectors\n",
        "        self.iter = iter\n",
        "\n",
        "    def update_clusters(self):\n",
        "        \"\"\"Determine which cluster center each `self.vector` is closest to.\"\"\"\n",
        "        def closest_center_index(vector):\n",
        "            \"\"\"Get the index of the closest cluster center to `self.vector`.\"\"\"\n",
        "            similarity_to_vector = lambda center: similarity(center,vector)\n",
        "            center = max(self.centers, key=similarity_to_vector)\n",
        "            return self.centers.index(center)\n",
        "\n",
        "        self.clusters = [[] for c in self.centers]\n",
        "        for vector in self.vectors:\n",
        "             index = closest_center_index(vector)\n",
        "             self.clusters[index].append(vector)\n",
        "\n",
        "    def update_centers(self):\n",
        "        \"\"\"Move `self.centers` to the centers of `self.clusters`.\n",
        "\n",
        "        Return True if centers moved, else False.\n",
        "\n",
        "        \"\"\"\n",
        "        new_centers = []\n",
        "        for cluster in self.clusters:\n",
        "            center = [average(ci) for ci in zip(*cluster)]\n",
        "            new_centers.append(center)\n",
        "\n",
        "        if new_centers == self.centers:\n",
        "            return False\n",
        "\n",
        "        self.centers = new_centers\n",
        "        return True\n",
        "\n",
        "    def main_loop(self):\n",
        "        \"\"\"Perform k-means clustering.\"\"\"\n",
        "        self.update_clusters()\n",
        "        # i = 0\n",
        "        # while i < self.iter:\n",
        "        #     self.update_clusters()\n",
        "        #     i += 1\n",
        "        while self.update_centers():\n",
        "            self.update_clusters()\n",
        "\n",
        "\n",
        "def average(sequence):\n",
        "    return sum(sequence) / len(sequence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmCp-5vjbvKr",
        "cellView": "form"
      },
      "source": [
        "#@title Apriori Algorithm { form-width: \"200px\" }\n",
        "import sys\n",
        "from itertools import chain, combinations\n",
        "from collections import defaultdict\n",
        "\n",
        "def subsets(arr):\n",
        "    \"\"\" Returns non empty subsets of arr\"\"\"\n",
        "    return chain(*[combinations(arr, i + 1) for i, a in enumerate(arr)])\n",
        "\n",
        "\n",
        "def returnItemsWithMinSupport(itemSet, transactionList, minSupport, freqSet):\n",
        "        \"\"\"calculates the support for items in the itemSet and returns a subset\n",
        "       of the itemSet each of whose elements satisfies the minimum support\"\"\"\n",
        "        _itemSet = set()\n",
        "        localSet = defaultdict(int)\n",
        "\n",
        "        for item in itemSet:\n",
        "                for transaction in transactionList:\n",
        "                        if item.issubset(transaction):\n",
        "                                freqSet[item] += 1\n",
        "                                localSet[item] += 1\n",
        "\n",
        "        for item, count in localSet.items():\n",
        "                support = float(count)/len(transactionList)\n",
        "\n",
        "                if support >= minSupport:\n",
        "                        _itemSet.add(item)\n",
        "\n",
        "        return _itemSet\n",
        "\n",
        "\n",
        "def joinSet(itemSet, length):\n",
        "        \"\"\"Join a set with itself and returns the n-element itemsets\"\"\"\n",
        "        return set([i.union(j) for i in itemSet for j in itemSet if len(i.union(j)) == length])\n",
        "\n",
        "\n",
        "def getItemSetTransactionList(data_iterator):\n",
        "    transactionList = list()\n",
        "    itemSet = set()\n",
        "    for record in data_iterator:\n",
        "        # record = record.split()\n",
        "        transaction = frozenset(record)\n",
        "        transactionList.append(transaction)\n",
        "        for item in transaction:\n",
        "            itemSet.add(frozenset([item]))              # Generate 1-itemSets\n",
        "    return itemSet, transactionList\n",
        "\n",
        "\n",
        "def runApriori(data_iter, original_text, minSupport=0.15, minConfidence=0.6):\n",
        "    \"\"\"\n",
        "    run the apriori algorithm. data_iter is a record iterator\n",
        "    Return both:\n",
        "     - items (tuple, support)\n",
        "     - rules ((pretuple, posttuple), confidence)\n",
        "    \"\"\"\n",
        "    itemSet, transactionList = getItemSetTransactionList(data_iter)\n",
        "\n",
        "    freqSet = defaultdict(int)\n",
        "    largeSet = dict()\n",
        "    # Global dictionary which stores (key=n-itemSets,value=support)\n",
        "    # which satisfy minSupport\n",
        "\n",
        "    # assocRules = dict()\n",
        "    # Dictionary which stores Association Rules\n",
        "\n",
        "    oneCSet = returnItemsWithMinSupport(itemSet,\n",
        "                                        transactionList,\n",
        "                                        minSupport,\n",
        "                                        freqSet)\n",
        "\n",
        "    currentLSet = oneCSet\n",
        "    k = 2\n",
        "    while(currentLSet != set([])):\n",
        "        largeSet[k-1] = currentLSet\n",
        "        currentLSet = joinSet(currentLSet, k)\n",
        "        currentCSet = returnItemsWithMinSupport(currentLSet,\n",
        "                                                transactionList,\n",
        "                                                minSupport,\n",
        "                                                freqSet)\n",
        "        currentLSet = currentCSet\n",
        "        k = k + 1\n",
        "\n",
        "    def getSupport(item):\n",
        "            \"\"\"local function which Returns the support of an item\"\"\"\n",
        "            return float(freqSet[item])/len(transactionList)\n",
        "\n",
        "    toRetItems = []\n",
        "    for key, value in largeSet.items():\n",
        "        toRetItems.extend([(tuple(item), getSupport(item))\n",
        "                           for item in value])\n",
        "\n",
        "    # toRetRules = []\n",
        "    # for key, value in list(largeSet.items())[1:]:\n",
        "    #     for item in value:\n",
        "    #         _subsets = map(frozenset, [x for x in subsets(item)])\n",
        "    #         for element in _subsets:\n",
        "    #             remain = item.difference(element)\n",
        "    #             if len(remain) > 0:\n",
        "    #                 confidence = getSupport(item)/getSupport(element)\n",
        "    #                 if confidence >= minConfidence:\n",
        "    #                     toRetRules.append(((tuple(element), tuple(remain)),\n",
        "    #                                        confidence))\n",
        "    return toRetItems#, toRetRules"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqLBzQK_9BXz",
        "cellView": "form"
      },
      "source": [
        "#@title Sentences evalution and selection { form-width: \"200px\" }\n",
        "def searchInItemSet(j, items):\n",
        "  for item, support in items:\n",
        "    if set(item) == set(j):\n",
        "      return support, True\n",
        "  return 0, False\n",
        "\n",
        "# cluster_senteces - actual sentences with the order number\n",
        "# sentences - concepts_list \n",
        "def getSummary(cluster_sentences, sentences, items, needed_length):\n",
        "\t# from itertools import combinations\n",
        "  sentencesScore = []\n",
        "  for sentence in sentences:\n",
        "    words = sentence #sentence.split()\n",
        "    sentenceScore = 0\n",
        "    for item, support in items:\n",
        "      if set(item).issubset(set(words)):\n",
        "        sentenceScore += len(item)*support\n",
        "    sentencesScore.append(sentenceScore)\n",
        "    #     return support, True\n",
        "\t\t# for i in range(1, len(words)):\n",
        "\t\t# \tcomb = combinations(words, i)\n",
        "\t\t# \tfor j in list(comb):\n",
        "\t\t# \t\tsupport, found = searchInItemSet(j,items)\n",
        "\t\t# \t\tif found:\n",
        "\t\t# \t\t\tsentenceScore += len(j)*support\n",
        "\t\t# sentencesScore.append(sentenceScore)\n",
        "  # neededLength = int(compression_ratio * len(sentences))\n",
        "\n",
        "  # sort the sentences according to the score and select the top sentences according to the compression ratio\n",
        "  selectedSentences = sorted(list(zip(cluster_sentences, sentencesScore)), key = lambda val: val[1], reverse = True)[:needed_length]\n",
        "\n",
        "  # since the selected sentences are not in the correct sequence we need to sort the selectedSentences in the order of the enumaration\n",
        "  selectedSentences.sort(key = lambda val: val[0][0])\n",
        "\n",
        "  topKSentences = []\n",
        "  for sntnce, _ in selectedSentences:\n",
        "    topKSentences.append(sntnce)\n",
        "  # return the final summary\n",
        "  return topKSentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFwB91ZOC5nv",
        "cellView": "form"
      },
      "source": [
        "#@title Rouge Evaluation Setup { form-width: \"200px\" }\n",
        "import pandas as pd\n",
        "\n",
        "columns = ['name', 'f', 'p', 'r', 'text']\n",
        "\n",
        "\n",
        "def gen_serie(name, rouge, text, version = 'rouge-l'):\n",
        "    return pd.Series([\n",
        "        name,\n",
        "        rouge[0][version]['f'],\n",
        "        rouge[0][version]['p'],\n",
        "        rouge[0][version]['r'],\n",
        "        text],\n",
        "        index=columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_a73mHUxQa8",
        "cellView": "form"
      },
      "source": [
        "#@title Proposed Algorithm { form-width: \"200px\" }\n",
        "import warnings\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.sparse\n",
        "from rouge import Rouge\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import pairwise_distances_argmin_min\n",
        "import re\n",
        "# from formatting import gen_serie\n",
        "# from tokenizer import textblob_tokenizer, tokenize\n",
        " \n",
        "warnings.filterwarnings(\"ignore\")\n",
        " \n",
        " \n",
        "def k_mean_distance(data, cx, cy, i_centroid, cluster_labels):\n",
        "    distances = [np.sqrt((x - cx) ** 2 + (y - cy) ** 2) for (x, y) in data[cluster_labels == i_centroid]]\n",
        "    return distances\n",
        " \n",
        "def remove_url(message):\n",
        "    return re.sub(r\"http\\S+\", \"\", message)\n",
        " \n",
        "def remove_citation(message):\n",
        "    return re.sub(r\"\\[([0-9])*(â€“)?([0-9])*\\]\", \"\", message)\n",
        " \n",
        "def remove_small_len(texts):\n",
        "    arr = []\n",
        "    for text in texts:\n",
        "        if len(text.split()) >= 4:\n",
        "            arr.append(text)\n",
        "    return arr\n",
        " \n",
        "def remove_stopwords(sentences):\n",
        "    filtered_sentences = []\n",
        "    stop_words = set(stopwords.words('english')) \n",
        "    for sentence in sentences:\n",
        "        word_tokens = word_tokenize(sentence) \n",
        "        filtered_sentence = [] \n",
        "        for w in word_tokens: \n",
        "            if w not in stop_words: \n",
        "                filtered_sentence.append(w)\n",
        "        filtered_sentences.append(\" \".join(filtered_sentence))\n",
        "    return filtered_sentences\n",
        " \n",
        "def lt(sent):\n",
        "    l = len(sent)\n",
        "    if l > 250:\n",
        "        import math\n",
        "        k = math.ceil(l/250)\n",
        "        sent = sent[::k]\n",
        "    return sent\n",
        " \n",
        "def delete_row_lil(mat, i):\n",
        "    if not isinstance(mat, scipy.sparse.lil_matrix):\n",
        "        raise ValueError(\"works only for LIL format -- use .tolil() first\")\n",
        "    mat.rows = np.delete(mat.rows, i)\n",
        "    mat.data = np.delete(mat.data, i)\n",
        "    mat._shape = (mat._shape[0] - 1, mat._shape[1])\n",
        "    return mat\n",
        " \n",
        " \n",
        "def cluster(matrix, clusters_nb):\n",
        "    # create a map between the sentence vector and the sentence so that we can refer to sentence\n",
        "    paragraph_map = dict(zip(map(str, matrix), [i for i in range(len(matrix))]))\n",
        "    # print(\"matrix_len: \", len(matrix))\n",
        "    #3.1.4\n",
        "    # tokenize the sentences to words and remove stop words\n",
        "    # vec = TfidfVectorizer(tokenizer=textblob_tokenizer,\n",
        "    #                       stop_words='english',\n",
        "    #                       use_idf=True)\n",
        "    # matrix = vec.fit_transform(texts)\n",
        "    # print(matrix)\n",
        " \n",
        "    # Normal Method - Euclidean Distance\n",
        "    # km = KMeans(n_clusters=clusters_nb, max_iter=10000, init='k-means++').fit(matrix)\n",
        "    # centres are km.cluster_centers_\n",
        " \n",
        "    # Cosine Similarity\n",
        "    k_means = CosineKMeans(clusters_nb, matrix, 1000)\n",
        "    print(\"Running Kmeans algorithm with cosine similarity\")\n",
        "    k_means.main_loop()\n",
        " \n",
        "    def translator(clusters, paragraph_map):\n",
        "        \"\"\"Translate vectors back into paragraphs, to make them human-readable.\"\"\"\n",
        "        def item_translator(vector):\n",
        "            return paragraph_map.get(str(vector))\n",
        " \n",
        "        def cluster_translator(cluster):\n",
        "            return list(map(item_translator, cluster))\n",
        " \n",
        "        return list(map(cluster_translator, clusters))\n",
        " \n",
        "    # here the paragraph map is the dict\n",
        "    # key: string of vectors of the tokens\n",
        "    # value: the sentence corresponding to that vector\n",
        "    clusters = translator(k_means.clusters, paragraph_map)\n",
        " \n",
        "    return clusters\n",
        " \n",
        " \n",
        "def our_method(raw, ref):\n",
        "    rougel = pd.DataFrame()\n",
        "    rouge1 = pd.DataFrame()\n",
        "    rouge2 = pd.DataFrame()\n",
        " \n",
        "    # data preprocessing\n",
        " \n",
        "    # remove url\n",
        "    sents = remove_url(raw)\n",
        " \n",
        "    # split text to sentence\n",
        "    sents = tokenize(raw)\n",
        " \n",
        "    # remove sentences of small len\n",
        "    sents = remove_small_len(sents)\n",
        " \n",
        "    # limit sentence\n",
        "    sents = lt(sents)\n",
        "    # sents = sents[::2]\n",
        " \n",
        "    orig_sent = list(enumerate(sents))\n",
        "    document_length = len(sents)\n",
        " \n",
        "    print(\"Length of the entire document: \", document_length)\n",
        "    \n",
        " \n",
        "    # remove citations from text\n",
        "    sents = list(map(remove_citation, sents))\n",
        " \n",
        "    # remove punctuation and convert to lower case\n",
        "    sents = remove_punctuation_to_lower_case(sents)\n",
        "    \n",
        "    # remove stop words\n",
        "    sents = remove_stopwords(sents)\n",
        "    print(\"Length of formatted sentences: \", len(sents))\n",
        "    print(\"Sample formatted sentences: \", sents[:2])\n",
        " \n",
        "    # UMLS Concept Mapping\n",
        "    # Bag-of-words model: Each sentence has list of concepts associated with it\n",
        "    print(\"Extracting biomedical concepts using Metamap\")\n",
        "    from pymetamap import MetaMap\n",
        "    mm = MetaMap.get_instance('/content/public_mm/bin/metamap16')\n",
        " \n",
        "    from collections import defaultdict, Counter\n",
        "    concepts = defaultdict(list)\n",
        "    all_concepts = [] # helper to calculate Sentence Frequency\n",
        "    # turn on word sense disambiguation to remove ambiguity\n",
        " \n",
        "    # ignored concepts - qualitative concept, quantitative concept, temporal concept, functional concept, idea or concept, intellectual product, mental process, spatial concept, and language\n",
        "    # https://metamap.nlm.nih.gov/Docs/SemanticTypes_2018AB.txt\n",
        "    ignrd_ctps = set(['qlco', 'qnco', 'tmco', 'ftcn', 'idcn', 'inpr', 'menp', 'spco', 'lang'])\n",
        " \n",
        "    # why dint u do for entire para: metamap doesnt work when length of para is too big\n",
        "    import ast \n",
        "    for idx, sentence in enumerate(sents):\n",
        "        ctpts,error = mm.extract_concepts([sentence],[0], word_sense_disambiguation=False)\n",
        "        for concept in ctpts:\n",
        "            b = re.sub(r\"([a-z]+)\",\"\\'\\\\1\\'\", concept.semtypes)\n",
        "            res = ast.literal_eval(b) \n",
        "            if not set(res).intersection(ignrd_ctps): \n",
        "                # print(idx, concept.semtypes)\n",
        "                concepts[idx].append(concept.preferred_name)\n",
        "                all_concepts.append(concept.preferred_name)\n",
        "            # IMPROVEMENT: concepts[concept.index].append((concept.preferred_name, concept.score)\n",
        " \n",
        "    print(\"\\n\\nSample Concepts found: \")\n",
        "    for i in range(3):\n",
        "        print(\"\\tSentence \", i, \": \", concepts[i])\n",
        " \n",
        " \n",
        "    print(\"Biomedical Concepts found in \", len(concepts), \" sentences\")\n",
        " \n",
        "    concepts_list = []\n",
        "    for i in range(len(orig_sent)):\n",
        "        if concepts[i]:\n",
        "          concepts_list.append(concepts[i])\n",
        "        else:\n",
        "          concepts_list.append([])\n",
        " \n",
        "    all_concepts_set = set(all_concepts)\n",
        "    print(\"Total number of unique concepts found: \", len(all_concepts_set))\n",
        "    all_concepts_dict = {k: v for v, k in enumerate(list(all_concepts_set))}\n",
        "    matrix = [[0]*len(all_concepts_set) for _ in range(document_length)]\n",
        " \n",
        " \n",
        "    print(\"Scoring the concepts\")\n",
        "    # Score( of each concept) = Concept Frequency * Sentence Frequency\n",
        "    # Concept Frequency is the proportion of the number of occurrences of the \n",
        "    # concept Ci in the sentence Sj to the total number of concepts in the same sentence Sj :\n",
        "    # Sentence frequency: Number of sentences having concept C / Total number of sentences\n",
        "    total_number_of_sentences = document_length\n",
        "    scored_concepts = defaultdict(dict)\n",
        "    all_concepts = Counter(all_concepts)\n",
        "    for i in concepts:\n",
        "        total_number_concepts_in_sentence = len(concepts[i])\n",
        "        counter = Counter(concepts[i])\n",
        "        for j in concepts[i]:\n",
        "            concept_frequency = counter[j]/total_number_concepts_in_sentence\n",
        "            sentence_frequency = all_concepts[j]/total_number_of_sentences\n",
        "            score = concept_frequency * sentence_frequency\n",
        "            scored_concepts[i][j] = score\n",
        "    # print(\"SCORED CONCEPTS\", scored_concepts)\n",
        " \n",
        "    # print(all_concepts_dict)\n",
        " \n",
        " \n",
        "    for i in scored_concepts:\n",
        "        for j in scored_concepts[i]:\n",
        "              matrix[int(i)][all_concepts_dict[j]] = scored_concepts[i][j]\n",
        "    # print(matrix)\n",
        " \n",
        "    print(\"COMPRESSION RATIO  ======\", len(tokenize(ref))/len(sents), \"=======\")\n",
        " \n",
        "    # nb_clusters = 4\n",
        "    for nb_clusters in range(3,8):\n",
        "        print(\"\".join([\"-\"]*50))\n",
        "        print(\"Starting Clustering\")\n",
        "        clusters_with_index = cluster(matrix, nb_clusters)\n",
        "        print(\"Clustering Done, number of clusters:: \", len(clusters_with_index))\n",
        "        # print(clusters_with_index)\n",
        "        # print(len(sents))\n",
        "        # change cluster number to the cluster sentences\n",
        "        clusters = []\n",
        "        for i in clusters_with_index:\n",
        "            temp = []\n",
        "            for j in i:\n",
        "                temp.append(concepts_list[j])\n",
        "            clusters.append(temp)\n",
        "        \n",
        "        print(\"Starting Apriori for each cluster\")\n",
        " \n",
        "        f_res = \"\"\n",
        "        for idx, cluster_ in enumerate(clusters):\n",
        "            # items, rules = runApriori(cluster_, sents)\n",
        "            items = runApriori(cluster_, sents)\n",
        "            print(\"Apriori done for cluster \", idx)\n",
        " \n",
        "            cluster_sentences = []\n",
        "            for i in clusters_with_index[idx]:\n",
        "                cluster_sentences.append([i, orig_sent[i][1]])\n",
        " \n",
        "            res = getSummary(cluster_sentences,cluster_, items, int((len(tokenize(ref))/len(sents))*len(cluster_)))\n",
        "            res.sort(key = lambda val:val[0])\n",
        " \n",
        "            # print(res)\n",
        "            \n",
        "            for i, s in res:\n",
        "                f_res += s  \n",
        "        \n",
        "        r = Rouge()\n",
        "        \n",
        "        # write summary to file\n",
        "        f = open(\"/content/our_summary\"+str(nb_clusters)+\".txt\",\"w\")\n",
        "        f.write(f_res)\n",
        "        f.close()\n",
        " \n",
        "        rouge = r.get_scores(f_res, ref)\n",
        "        rougel = rougel.append(gen_serie('Our method('+str(nb_clusters)+') ', rouge, res,version = 'rouge-l'), ignore_index=True)\n",
        "        rouge1 = rouge1.append(gen_serie('Our method('+str(nb_clusters)+') ', rouge, res,version = 'rouge-1'), ignore_index=True)\n",
        "        rouge2 = rouge2.append(gen_serie('Our method('+str(nb_clusters)+') ', rouge, res,version = 'rouge-2'), ignore_index=True)\n",
        "        \n",
        "    return rougel, rouge1, rouge2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmYBusE8xwNQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "2f037240-7de8-4f92-c084-050a3601dabb"
      },
      "source": [
        "#@title Evalution of All methods { form-width: \"200px\" }\n",
        "import pandas as pd\n",
        "from gensim.summarization import summarize\n",
        "from rouge import Rouge\n",
        "import sys\n",
        "sys.setrecursionlimit(100000)\n",
        "\n",
        "# from cosine_similarity import cosine\n",
        "# from formatting import gen_serie, columns\n",
        "# from kmean import kmean\n",
        "# from scraper import get_article\n",
        "# from text_rank import text_rank\n",
        "# from tokenizer import tokenize\n",
        "last_time = time.time()\n",
        "\n",
        "def computeal(topic):\n",
        "    raw, ref = get_article(topic)\n",
        "\n",
        "    # tokenize the paras into sentences\n",
        "    sent = tokenize(raw)\n",
        "    sent = lt(sent)\n",
        "    \n",
        "\n",
        "    rougel = pd.DataFrame()\n",
        "    rouge1 = pd.DataFrame()\n",
        "    rouge2 = pd.DataFrame()\n",
        "\n",
        "    ratio = len(ref) / len(raw)\n",
        "\n",
        "    # KMean\n",
        "    kmean_res = our_method(raw, ref)\n",
        "    rougel = rougel.append(kmean_res[0])\n",
        "    rouge1 = rouge1.append(kmean_res[1])\n",
        "    rouge2 = rouge2.append(kmean_res[2])\n",
        "\n",
        "    # TextRank\n",
        "    result = text_rank(raw, sent, ref)\n",
        "    r = Rouge()\n",
        "    rouge = r.get_scores(result, ref)\n",
        "    rougel = rougel.append(gen_serie('TextRank', rouge, result,version = 'rouge-l'), ignore_index=True)\n",
        "    rouge1 = rouge1.append(gen_serie('TextRank', rouge, result,version = 'rouge-1'), ignore_index=True)\n",
        "    rouge2 = rouge2.append(gen_serie('TextRank', rouge, result,version = 'rouge-2'), ignore_index=True)\n",
        "\n",
        "    # Gensim\n",
        "    ret = summarize(raw, ratio)\n",
        "    r = Rouge()\n",
        "    rouge = r.get_scores(ret, ref)\n",
        "    rougel = rougel.append(gen_serie('Gensim', rouge, ret,version = 'rouge-l'), ignore_index=True)\n",
        "    rouge1 = rouge1.append(gen_serie('Gensim', rouge, ret,version = 'rouge-1'), ignore_index=True)\n",
        "    rouge2 = rouge2.append(gen_serie('Gensim', rouge, ret,version = 'rouge-2'), ignore_index=True)\n",
        "\n",
        "    # Cosine\n",
        "    res = cosine(sent, ref)\n",
        "    rougel = rougel.append(res[0], ignore_index=True)\n",
        "    rouge1 = rouge1.append(res[1], ignore_index=True)\n",
        "    rouge2 = rouge2.append(res[2], ignore_index=True)\n",
        "\n",
        "    \n",
        "    print(\"========= Text summarization Complete =========\")\n",
        "\n",
        "    # Rearrange columns\n",
        "    rougel = rougel[columns]\n",
        "    del rougel['text']\n",
        "    rougel.to_csv('/content/output_rougel.csv')\n",
        "    print(\"\\n========= ROUGE L =========\")\n",
        "    print(rougel)\n",
        "\n",
        "\n",
        "    # Rearrange columns\n",
        "    rouge1 = rouge1[columns]\n",
        "    del rouge1['text']\n",
        "    rouge1.to_csv('/content/output_rouge1.csv')\n",
        "    print(\"\\n========= ROUGE 1 =========\")\n",
        "    print(rouge1)\n",
        "\n",
        "    # Rearrange columns\n",
        "    rouge2 = rouge2[columns]\n",
        "    del rouge2['text']\n",
        "    rougel.to_csv('/content/output_rouge2.csv')\n",
        "    print(\"\\n========= ROUGE 2 =========\")\n",
        "    print(rouge2)\n",
        "\n",
        "\n",
        "    # return df.to_json(orient='records'), df\n",
        "\n",
        "computeal(\"heart\")\n",
        "\n",
        "print(\"\".join([\"-\"]*50))\n",
        "print(\"Total time taken: \", int((time.time()-last_time)/60), \" minutes and \", (time.time()-last_time)%60,\" seconds\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of the entire document:  168\n",
            "Length of formatted sentences:  168\n",
            "Sample formatted sentences:  ['heart muscular organ animals pumps blood blood vessels circulatory system', 'commonly right atrium ventricle referred together right heart left counterparts left heart']\n",
            "Extracting biomedical concepts using Metamap\n",
            "\n",
            "\n",
            "Sample Concepts found: \n",
            "\tSentence  0 :  ['Blood Circulation', 'Myocardium', 'Animals', 'Muscle', 'Heart', 'Blood Vessel', 'Pump Device Component', 'pump (device)', 'Vascular System', 'animal allergen extracts', 'Entire heart', 'Organ']\n",
            "\tSentence  1 :  ['Heart Ventricle', 'Heart Atrium', 'Entire right ventricle', 'Right ventricular structure', 'Ventricle', 'Entire right atrium', 'Right atrial structure', 'Right side of heart', 'Entire atrium']\n",
            "\tSentence  2 :  ['Pericardial sac structure', 'Heart', 'Body Fluids', 'Contain (action)', 'Entire heart', 'Entire pericardium', 'Liquid substance', 'fluid - substance', 'ADCY10 gene', 'spindle assembly checkpoint', 'subapical complex location']\n",
            "Biomedical Concepts found in  152  sentences\n",
            "Total number of unique concepts found:  867\n",
            "Scoring the concepts\n",
            "COMPRESSION RATIO  ====== 0.1130952380952381 =======\n",
            "--------------------------------------------------\n",
            "Starting Clustering\n",
            "Running Kmeans algorithm with cosine similarity\n",
            "Clustering Done, number of clusters::  3\n",
            "Starting Apriori for each cluster\n",
            "Apriori done for cluster  0\n",
            "Apriori done for cluster  1\n",
            "Apriori done for cluster  2\n",
            "--------------------------------------------------\n",
            "Starting Clustering\n",
            "Running Kmeans algorithm with cosine similarity\n",
            "Clustering Done, number of clusters::  4\n",
            "Starting Apriori for each cluster\n",
            "Apriori done for cluster  0\n",
            "Apriori done for cluster  1\n",
            "Apriori done for cluster  2\n",
            "Apriori done for cluster  3\n",
            "--------------------------------------------------\n",
            "Starting Clustering\n",
            "Running Kmeans algorithm with cosine similarity\n",
            "Clustering Done, number of clusters::  5\n",
            "Starting Apriori for each cluster\n",
            "Apriori done for cluster  0\n",
            "Apriori done for cluster  1\n",
            "Apriori done for cluster  2\n",
            "Apriori done for cluster  3\n",
            "Apriori done for cluster  4\n",
            "--------------------------------------------------\n",
            "Starting Clustering\n",
            "Running Kmeans algorithm with cosine similarity\n",
            "Clustering Done, number of clusters::  6\n",
            "Starting Apriori for each cluster\n",
            "Apriori done for cluster  0\n",
            "Apriori done for cluster  1\n",
            "Apriori done for cluster  2\n",
            "Apriori done for cluster  3\n",
            "Apriori done for cluster  4\n",
            "Apriori done for cluster  5\n",
            "--------------------------------------------------\n",
            "Starting Clustering\n",
            "Running Kmeans algorithm with cosine similarity\n",
            "Clustering Done, number of clusters::  7\n",
            "Starting Apriori for each cluster\n",
            "Apriori done for cluster  0\n",
            "Apriori done for cluster  1\n",
            "Apriori done for cluster  2\n",
            "Apriori done for cluster  3\n",
            "Apriori done for cluster  4\n",
            "Apriori done for cluster  5\n",
            "Apriori done for cluster  6\n",
            "========= Text summarization Complete =========\n",
            "\n",
            "========= ROUGE L =========\n",
            "                name         f         p         r\n",
            "0     Our method(3)   0.497608  0.574586  0.438819\n",
            "1     Our method(4)   0.487696  0.519048  0.459916\n",
            "2     Our method(5)   0.427948  0.443439  0.413502\n",
            "3     Our method(6)   0.383838  0.477987  0.320675\n",
            "4     Our method(7)   0.383838  0.477987  0.320675\n",
            "5           TextRank  0.338710  0.324324  0.354430\n",
            "6             Gensim  0.429942  0.394366  0.472574\n",
            "7  Cosine Similarity  0.247059  0.230769  0.265823\n",
            "\n",
            "========= ROUGE 1 =========\n",
            "                name         f         p         r\n",
            "0     Our method(3)   0.534194  0.633028  0.462054\n",
            "1     Our method(4)   0.579611  0.595294  0.564732\n",
            "2     Our method(5)   0.549828  0.564706  0.535714\n",
            "3     Our method(6)   0.492715  0.605863  0.415179\n",
            "4     Our method(7)   0.492715  0.605863  0.415179\n",
            "5           TextRank  0.534161  0.498069  0.575893\n",
            "6             Gensim  0.516464  0.422096  0.665179\n",
            "7  Cosine Similarity  0.461053  0.436255  0.488839\n",
            "\n",
            "========= ROUGE 2 =========\n",
            "                name         f         p         r\n",
            "0     Our method(3)   0.354463  0.420245  0.306488\n",
            "1     Our method(4)   0.390356  0.400943  0.380313\n",
            "2     Our method(5)   0.305396  0.313679  0.297539\n",
            "3     Our method(6)   0.284197  0.349673  0.239374\n",
            "4     Our method(7)   0.284197  0.349673  0.239374\n",
            "5           TextRank  0.238589  0.222437  0.257271\n",
            "6             Gensim  0.328125  0.268085  0.422819\n",
            "7  Cosine Similarity  0.145570  0.137725  0.154362\n",
            "--------------------------------------------------\n",
            "Total time taken:  16  minutes and  9.771442890167236  seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aItTk4MsJFag"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}